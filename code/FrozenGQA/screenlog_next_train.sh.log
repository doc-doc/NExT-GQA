*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
ERROR:root:No token file found. Also make sure that a [prod] section with a 'token = value' assignment exists.
ERROR:root:No token file found. Also make sure that a [prod] section with a 'token = value' assignment exists.
ERROR:root:No token file found. Also make sure that a [prod] section with a 'token = value' assignment exists.
ERROR:root:No token file found. Also make sure that a [prod] section with a 'token = value' assignment exists.
| distributed init (rank 0): env://
| distributed init (rank 3): env://
| distributed init (rank 2): env://
| distributed init (rank 1): env://
Namespace(baseline='NG', batch_size=6, batch_size_val=64, beta1=0.9, beta2=0.95, clip_max_norm=0.1, combine_datasets=['nextgqa'], combine_datasets_val=['nextgqa'], device='cuda', dist_backend='nccl', dist_url='env://', distributed=True, dropout=0.1, ds_factor_attn=8, ds_factor_ff=8, epochs=20, eval=False, eval_skip=1, feat_type='CLIPL', features_dim=768, fraction_warmup_steps=0.1, freeze_last=True, freeze_lm=True, freeze_mlm=True, ft_ln=True, gamma=0.8, gpu=0, load='../../../data/pretrain_models/FBLM/frozenbilm.pth', lr=1e-05, lr_drop=10, max_atokens=5, max_feats=8, max_tokens=100, mlm_prob=0.15, model_name='../../../data/pretrain_models/deberta-v2-xlarge', n_ans=0, nextgqa_features_path='../../../data/nextgqa/', nextgqa_test_csv_path='../../datasets/nextgqa/test.csv', nextgqa_train_csv_path='../../datasets/nextgqa/train.csv', nextgqa_val_csv_path='../../datasets/nextgqa/val.csv', nextqa_features_path='../../datasets/nextqa/clipvitl14.pth', nextqa_test_csv_path='../../datasets/nextqa/test.csv', nextqa_train_csv_path='../../datasets/nextqa/train.csv', nextqa_val_csv_path='../../datasets/nextqa/val.csv', num_workers=3, optimizer='adam', prefix='', presave_dir='', print_freq=2000, question_example='', rank=0, resume=False, save_dir='../../../data/gmodels/nextgqa/new/FBLM/', schedule='linear_with_warmup', scratch=False, seed=42, sigma=9.0, start_epoch=0, suffix='.', test=False, use_context=True, use_video=True, vg_loss=0.0, video_example='', weight_decay=0, world_size=4)
../../../data/pretrain_models/deberta-v2-xlarge
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at ../../../data/pretrain_models/deberta-v2-xlarge and are newly initialized: ['deberta.encoder.layer.8.output.adapter.down.bias', 'trans_gauss.layer.0.attention.k_lin.weight', 'deberta.encoder.layer.4.output.adapter.up.weight', 'deberta.encoder.layer.4.attention.output.adapter.down.bias', 'deberta.encoder.layer.18.attention.output.adapter.down.weight', 'deberta.encoder.layer.20.attention.output.adapter.up.weight', 'trans_gauss.layer.0.sa_layer_norm.weight', 'deberta.encoder.layer.2.attention.output.adapter.down.bias', 'deberta.encoder.layer.7.output.adapter.down.weight', 'logit_gauss.1.bias', 'deberta.encoder.layer.20.attention.output.adapter.down.bias', 'deberta.encoder.layer.18.attention.output.adapter.down.bias', 'trans_gauss.layer.0.attention.q_lin.bias', 'deberta.encoder.layer.16.attention.output.adapter.down.bias', 'trans_gauss.layer.0.ffn.lin2.bias', 'deberta.encoder.layer.11.output.adapter.down.bias', 'deberta.encoder.layer.23.attention.output.adapter.down.bias', 'deberta.encoder.layer.23.output.adapter.down.bias', 'trans_gauss.layer.0.sa_layer_norm.bias', 'deberta.encoder.layer.7.attention.output.adapter.down.bias', 'deberta.encoder.layer.15.attention.output.adapter.up.bias', 'deberta.encoder.layer.19.attention.output.adapter.down.weight', 'deberta.encoder.layer.23.attention.output.adapter.up.bias', 'deberta.encoder.layer.3.output.adapter.down.weight', 'deberta.encoder.layer.11.attention.output.adapter.up.bias', 'trans_gauss.layer.0.output_layer_norm.bias', 'deberta.encoder.layer.17.output.adapter.up.bias', 'deberta.encoder.layer.6.output.adapter.up.bias', 'deberta.encoder.layer.21.attention.output.adapter.up.bias', 'deberta.encoder.layer.20.output.adapter.up.weight', 'deberta.encoder.layer.22.output.adapter.down.bias', 'deberta.encoder.layer.15.attention.output.adapter.up.weight', 'deberta.encoder.layer.6.attention.output.adapter.down.bias', 'deberta.encoder.layer.6.attention.output.adapter.up.bias', 'deberta.encoder.layer.0.output.adapter.down.bias', 'deberta.encoder.layer.0.attention.output.adapter.down.bias', 'deberta.encoder.layer.13.attention.output.adapter.up.weight', 'trans_gauss.layer.0.attention.out_lin.weight', 'deberta.encoder.layer.12.attention.output.adapter.down.bias', 'deberta.encoder.layer.9.attention.output.adapter.up.weight', 'deberta.encoder.layer.2.output.adapter.up.weight', 'deberta.encoder.layer.17.output.adapter.down.bias', 'deberta.encoder.layer.3.output.adapter.down.bias', 'deberta.encoder.layer.22.output.adapter.up.bias', 'deberta.encoder.layer.13.output.adapter.up.bias', 'deberta.encoder.layer.9.output.adapter.up.weight', 'deberta.encoder.layer.1.output.adapter.up.weight', 'deberta.encoder.layer.14.attention.output.adapter.up.bias', 'deberta.encoder.layer.14.attention.output.adapter.up.weight', 'deberta.encoder.layer.3.output.adapter.up.weight', 'deberta.encoder.layer.1.output.adapter.up.bias', 'deberta.encoder.layer.1.output.adapter.down.bias', 'answer_bias', 'deberta.encoder.layer.5.output.adapter.up.weight', 'deberta.encoder.layer.2.output.adapter.down.weight', 'logit_gauss.1.weight', 'deberta.encoder.layer.1.output.adapter.down.weight', 'deberta.encoder.layer.5.attention.output.adapter.up.weight', 'deberta.encoder.layer.9.attention.output.adapter.up.bias', 'deberta.encoder.layer.4.attention.output.adapter.up.weight', 'deberta.encoder.layer.19.output.adapter.down.weight', 'deberta.encoder.layer.23.attention.output.adapter.down.weight', 'deberta.encoder.layer.12.output.adapter.up.weight', 'deberta.encoder.layer.5.output.adapter.down.bias', 'deberta.encoder.layer.11.attention.output.adapter.down.bias', 'deberta.encoder.layer.18.output.adapter.up.weight', 'deberta.encoder.layer.22.attention.output.adapter.up.weight', 'lm_predictions.lm_head.decoder.weight', 'deberta.encoder.layer.0.output.adapter.up.weight', 'deberta.encoder.layer.1.attention.output.adapter.up.weight', 'deberta.encoder.layer.2.attention.output.adapter.down.weight', 'deberta.encoder.layer.16.attention.output.adapter.up.bias', 'deberta.encoder.layer.23.output.adapter.up.bias', 'deberta.encoder.layer.19.output.adapter.up.bias', 'deberta.encoder.layer.8.attention.output.adapter.down.weight', 'deberta.encoder.layer.8.attention.output.adapter.up.weight', 'deberta.encoder.layer.16.output.adapter.down.weight', 'deberta.encoder.layer.9.attention.output.adapter.down.bias', 'trans_gauss.layer.0.ffn.lin1.weight', 'deberta.encoder.layer.5.attention.output.adapter.up.bias', 'deberta.encoder.layer.8.output.adapter.up.weight', 'deberta.encoder.layer.8.output.adapter.up.bias', 'trans_gauss.layer.0.attention.v_lin.weight', 'deberta.encoder.layer.11.output.adapter.up.bias', 'deberta.encoder.layer.3.attention.output.adapter.up.weight', 'deberta.encoder.layer.6.output.adapter.up.weight', 'answer_embeddings.weight', 'deberta.encoder.layer.3.attention.output.adapter.down.weight', 'deberta.encoder.layer.21.attention.output.adapter.up.weight', 'deberta.encoder.layer.10.attention.output.adapter.down.bias', 'deberta.encoder.layer.21.output.adapter.down.weight', 'deberta.encoder.layer.13.attention.output.adapter.down.weight', 'deberta.encoder.layer.16.output.adapter.up.bias', 'deberta.encoder.layer.13.attention.output.adapter.up.bias', 'deberta.encoder.layer.2.output.adapter.up.bias', 'deberta.encoder.layer.0.output.adapter.up.bias', 'deberta.encoder.layer.3.attention.output.adapter.down.bias', 'deberta.encoder.layer.18.output.adapter.up.bias', 'deberta.encoder.layer.14.output.adapter.up.weight', 'deberta.encoder.layer.8.output.adapter.down.weight', 'trans_gauss.layer.0.ffn.lin1.bias', 'deberta.encoder.layer.2.attention.output.adapter.up.weight', 'deberta.encoder.layer.12.attention.output.adapter.down.weight', 'deberta.encoder.layer.22.attention.output.adapter.down.bias', 'deberta.encoder.layer.10.attention.output.adapter.up.weight', 'deberta.encoder.layer.13.output.adapter.up.weight', 'trans_gauss.layer.0.output_layer_norm.weight', 'deberta.encoder.layer.3.output.adapter.up.bias', 'deberta.encoder.layer.18.attention.output.adapter.up.weight', 'deberta.encoder.layer.7.output.adapter.up.bias', 'deberta.encoder.layer.21.attention.output.adapter.down.weight', 'deberta.encoder.layer.12.output.adapter.up.bias', 'deberta.encoder.layer.19.attention.output.adapter.up.weight', 'deberta.encoder.layer.15.attention.output.adapter.down.weight', 'deberta.encoder.layer.16.attention.output.adapter.down.weight', 'deberta.encoder.layer.20.attention.output.adapter.up.bias', 'deberta.encoder.layer.10.output.adapter.down.bias', 'deberta.encoder.layer.1.attention.output.adapter.up.bias', 'deberta.encoder.layer.6.output.adapter.down.weight', 'deberta.encoder.layer.11.output.adapter.down.weight', 'deberta.encoder.layer.5.attention.output.adapter.down.weight', 'deberta.encoder.layer.12.attention.output.adapter.up.bias', 'deberta.encoder.layer.7.attention.output.adapter.up.bias', 'deberta.encoder.layer.8.attention.output.adapter.up.bias', 'deberta.encoder.layer.15.output.adapter.up.bias', 'deberta.encoder.layer.7.attention.output.adapter.up.weight', 'trans_gauss.layer.0.ffn.lin2.weight', 'deberta.encoder.layer.11.attention.output.adapter.up.weight', 'deberta.encoder.layer.4.output.adapter.down.weight', 'deberta.encoder.layer.22.attention.output.adapter.down.weight', 'deberta.encoder.layer.15.attention.output.adapter.down.bias', 'deberta.encoder.layer.15.output.adapter.up.weight', 'deberta.encoder.layer.0.attention.output.adapter.up.weight', 'deberta.encoder.layer.18.output.adapter.down.weight', 'deberta.encoder.layer.21.attention.output.adapter.down.bias', 'deberta.encoder.layer.6.attention.output.adapter.up.weight', 'trans_gauss.layer.0.attention.q_lin.weight', 'trans_gauss.layer.0.attention.v_lin.bias', 'deberta.encoder.layer.10.output.adapter.down.weight', 'deberta.encoder.layer.7.attention.output.adapter.down.weight', 'deberta.encoder.layer.11.attention.output.adapter.down.weight', 'deberta.encoder.layer.12.output.adapter.down.weight', 'deberta.encoder.layer.19.output.adapter.up.weight', 'deberta.encoder.layer.10.output.adapter.up.bias', 'deberta.encoder.layer.14.output.adapter.down.weight', 'deberta.encoder.layer.17.output.adapter.down.weight', 'deberta.encoder.layer.9.attention.output.adapter.down.weight', 'deberta.encoder.layer.4.attention.output.adapter.up.bias', 'deberta.encoder.layer.5.output.adapter.down.weight', 'deberta.embeddings.linear_video.weight', 'deberta.encoder.layer.5.attention.output.adapter.down.bias', 'deberta.encoder.layer.0.attention.output.adapter.up.bias', 'deberta.encoder.layer.19.attention.output.adapter.down.bias', 'deberta.encoder.layer.4.output.adapter.down.bias', 'deberta.encoder.layer.19.output.adapter.down.bias', 'deberta.encoder.layer.2.attention.output.adapter.up.bias', 'deberta.encoder.layer.22.output.adapter.up.weight', 'deberta.encoder.layer.7.output.adapter.down.bias', 'deberta.encoder.layer.14.attention.output.adapter.down.bias', 'deberta.encoder.layer.16.output.adapter.down.bias', 'deberta.encoder.layer.22.output.adapter.down.weight', 'deberta.encoder.layer.1.attention.output.adapter.down.bias', 'deberta.encoder.layer.20.attention.output.adapter.down.weight', 'deberta.encoder.layer.4.output.adapter.up.bias', 'deberta.encoder.layer.23.output.adapter.down.weight', 'deberta.encoder.layer.13.output.adapter.down.bias', 'deberta.encoder.layer.17.attention.output.adapter.down.bias', 'deberta.encoder.layer.18.output.adapter.down.bias', 'deberta.encoder.layer.20.output.adapter.down.weight', 'deberta.encoder.layer.9.output.adapter.up.bias', 'deberta.encoder.layer.14.output.adapter.up.bias', 'deberta.encoder.layer.12.attention.output.adapter.up.weight', 'deberta.encoder.layer.21.output.adapter.down.bias', 'deberta.encoder.layer.21.output.adapter.up.weight', 'deberta.encoder.layer.23.output.adapter.up.weight', 'deberta.encoder.layer.14.output.adapter.down.bias', 'deberta.encoder.layer.15.output.adapter.down.bias', 'deberta.encoder.layer.23.attention.output.adapter.up.weight', 'deberta.encoder.layer.10.output.adapter.up.weight', 'lm_predictions.lm_head.decoder.bias', 'deberta.encoder.layer.14.attention.output.adapter.down.weight', 'deberta.encoder.layer.12.output.adapter.down.bias', 'deberta.encoder.layer.1.attention.output.adapter.down.weight', 'deberta.encoder.layer.5.output.adapter.up.bias', 'deberta.encoder.layer.15.output.adapter.down.weight', 'deberta.encoder.layer.16.output.adapter.up.weight', 'deberta.encoder.layer.17.attention.output.adapter.up.weight', 'deberta.encoder.layer.3.attention.output.adapter.up.bias', 'deberta.encoder.layer.18.attention.output.adapter.up.bias', 'deberta.encoder.layer.10.attention.output.adapter.down.weight', 'deberta.encoder.layer.7.output.adapter.up.weight', 'deberta.encoder.layer.0.output.adapter.down.weight', 'deberta.encoder.layer.19.attention.output.adapter.up.bias', 'deberta.encoder.layer.8.attention.output.adapter.down.bias', 'deberta.embeddings.linear_video.bias', 'deberta.encoder.layer.13.attention.output.adapter.down.bias', 'deberta.encoder.layer.20.output.adapter.up.bias', 'deberta.encoder.layer.11.output.adapter.up.weight', 'deberta.encoder.layer.0.attention.output.adapter.down.weight', 'deberta.encoder.layer.9.output.adapter.down.bias', 'deberta.encoder.layer.4.attention.output.adapter.down.weight', 'deberta.encoder.layer.21.output.adapter.up.bias', 'deberta.encoder.layer.13.output.adapter.down.weight', 'deberta.encoder.layer.20.output.adapter.down.bias', 'deberta.encoder.layer.17.attention.output.adapter.down.weight', 'deberta.encoder.layer.6.output.adapter.down.bias', 'deberta.encoder.layer.17.attention.output.adapter.up.bias', 'deberta.encoder.layer.22.attention.output.adapter.up.bias', 'deberta.encoder.layer.2.output.adapter.down.bias', 'deberta.encoder.layer.17.output.adapter.up.weight', 'trans_gauss.layer.0.attention.k_lin.bias', 'deberta.encoder.layer.16.attention.output.adapter.up.weight', 'deberta.encoder.layer.9.output.adapter.down.weight', 'trans_gauss.layer.0.attention.out_lin.bias', 'deberta.encoder.layer.10.attention.output.adapter.up.bias', 'deberta.encoder.layer.6.attention.output.adapter.down.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at ../../../data/pretrain_models/deberta-v2-xlarge and are newly initialized: ['deberta.encoder.layer.20.output.adapter.up.weight', 'deberta.encoder.layer.12.output.adapter.down.bias', 'deberta.encoder.layer.22.attention.output.adapter.up.weight', 'deberta.encoder.layer.4.attention.output.adapter.down.weight', 'deberta.encoder.layer.20.attention.output.adapter.up.weight', 'deberta.encoder.layer.11.output.adapter.down.bias', 'deberta.encoder.layer.16.attention.output.adapter.up.bias', 'deberta.encoder.layer.18.output.adapter.up.weight', 'deberta.encoder.layer.2.attention.output.adapter.down.weight', 'deberta.encoder.layer.13.output.adapter.up.bias', 'deberta.encoder.layer.0.attention.output.adapter.down.bias', 'deberta.encoder.layer.21.attention.output.adapter.down.weight', 'deberta.encoder.layer.20.attention.output.adapter.down.bias', 'deberta.encoder.layer.6.attention.output.adapter.up.bias', 'deberta.encoder.layer.8.attention.output.adapter.up.weight', 'deberta.encoder.layer.21.attention.output.adapter.up.bias', 'deberta.encoder.layer.9.attention.output.adapter.up.bias', 'deberta.encoder.layer.22.attention.output.adapter.up.bias', 'deberta.encoder.layer.9.attention.output.adapter.down.weight', 'deberta.encoder.layer.3.output.adapter.up.weight', 'deberta.encoder.layer.17.output.adapter.up.weight', 'deberta.encoder.layer.12.output.adapter.down.weight', 'deberta.encoder.layer.23.attention.output.adapter.down.bias', 'deberta.encoder.layer.8.attention.output.adapter.up.bias', 'deberta.encoder.layer.15.output.adapter.down.weight', 'deberta.encoder.layer.18.attention.output.adapter.down.bias', 'deberta.encoder.layer.8.output.adapter.up.bias', 'deberta.encoder.layer.16.output.adapter.up.bias', 'deberta.encoder.layer.16.output.adapter.up.weight', 'deberta.encoder.layer.5.output.adapter.down.bias', 'deberta.encoder.layer.20.attention.output.adapter.down.weight', 'deberta.encoder.layer.14.attention.output.adapter.up.weight', 'deberta.encoder.layer.19.attention.output.adapter.up.weight', 'deberta.encoder.layer.13.attention.output.adapter.down.weight', 'deberta.encoder.layer.19.output.adapter.down.bias', 'deberta.encoder.layer.7.output.adapter.down.bias', 'deberta.encoder.layer.15.attention.output.adapter.up.weight', 'deberta.encoder.layer.15.output.adapter.up.bias', 'deberta.encoder.layer.0.output.adapter.down.bias', 'deberta.encoder.layer.3.attention.output.adapter.up.weight', 'deberta.encoder.layer.6.output.adapter.up.weight', 'deberta.encoder.layer.18.output.adapter.down.bias', 'deberta.encoder.layer.14.output.adapter.down.bias', 'deberta.encoder.layer.1.output.adapter.up.weight', 'deberta.encoder.layer.16.attention.output.adapter.down.weight', 'deberta.encoder.layer.1.attention.output.adapter.up.weight', 'deberta.encoder.layer.23.output.adapter.up.weight', 'trans_gauss.layer.0.sa_layer_norm.weight', 'trans_gauss.layer.0.sa_layer_norm.bias', 'deberta.encoder.layer.5.attention.output.adapter.up.weight', 'deberta.encoder.layer.3.output.adapter.up.bias', 'deberta.encoder.layer.12.output.adapter.up.bias', 'deberta.encoder.layer.2.output.adapter.down.weight', 'trans_gauss.layer.0.attention.v_lin.weight', 'deberta.encoder.layer.18.attention.output.adapter.down.weight', 'deberta.encoder.layer.23.attention.output.adapter.down.weight', 'logit_gauss.1.weight', 'deberta.encoder.layer.13.attention.output.adapter.down.bias', 'deberta.encoder.layer.2.output.adapter.up.weight', 'deberta.encoder.layer.16.output.adapter.down.bias', 'deberta.encoder.layer.2.attention.output.adapter.up.bias', 'trans_gauss.layer.0.attention.out_lin.weight', 'deberta.encoder.layer.4.attention.output.adapter.up.weight', 'deberta.encoder.layer.9.output.adapter.up.bias', 'deberta.encoder.layer.15.attention.output.adapter.up.bias', 'deberta.encoder.layer.9.attention.output.adapter.up.weight', 'deberta.encoder.layer.8.output.adapter.down.weight', 'deberta.encoder.layer.13.attention.output.adapter.up.weight', 'deberta.encoder.layer.22.output.adapter.up.bias', 'deberta.encoder.layer.7.output.adapter.down.weight', 'deberta.encoder.layer.6.attention.output.adapter.down.bias', 'deberta.encoder.layer.13.output.adapter.down.bias', 'deberta.encoder.layer.1.attention.output.adapter.down.weight', 'deberta.encoder.layer.8.output.adapter.down.bias', 'deberta.encoder.layer.10.attention.output.adapter.up.bias', 'deberta.encoder.layer.15.attention.output.adapter.down.bias', 'deberta.encoder.layer.12.attention.output.adapter.down.weight', 'lm_predictions.lm_head.decoder.weight', 'deberta.encoder.layer.15.output.adapter.down.bias', 'deberta.encoder.layer.11.attention.output.adapter.down.bias', 'answer_bias', 'deberta.encoder.layer.10.output.adapter.up.bias', 'deberta.encoder.layer.6.attention.output.adapter.up.weight', 'deberta.encoder.layer.7.attention.output.adapter.up.bias', 'trans_gauss.layer.0.ffn.lin2.bias', 'deberta.encoder.layer.22.attention.output.adapter.down.weight', 'deberta.encoder.layer.4.attention.output.adapter.up.bias', 'deberta.encoder.layer.23.output.adapter.down.bias', 'deberta.encoder.layer.20.output.adapter.down.bias', 'deberta.encoder.layer.0.output.adapter.up.weight', 'deberta.encoder.layer.3.output.adapter.down.bias', 'deberta.encoder.layer.4.attention.output.adapter.down.bias', 'trans_gauss.layer.0.attention.k_lin.weight', 'deberta.encoder.layer.15.output.adapter.up.weight', 'deberta.encoder.layer.11.output.adapter.up.weight', 'trans_gauss.layer.0.attention.q_lin.bias', 'deberta.encoder.layer.9.output.adapter.down.bias', 'deberta.encoder.layer.9.attention.output.adapter.down.bias', 'deberta.encoder.layer.20.output.adapter.down.weight', 'deberta.encoder.layer.21.output.adapter.down.bias', 'deberta.encoder.layer.3.output.adapter.down.weight', 'deberta.encoder.layer.17.attention.output.adapter.down.weight', 'deberta.encoder.layer.7.attention.output.adapter.up.weight', 'deberta.encoder.layer.6.output.adapter.down.bias', 'deberta.encoder.layer.18.output.adapter.up.bias', 'deberta.encoder.layer.3.attention.output.adapter.down.weight', 'deberta.encoder.layer.4.output.adapter.down.bias', 'deberta.encoder.layer.22.output.adapter.up.weight', 'deberta.encoder.layer.5.attention.output.adapter.down.bias', 'trans_gauss.layer.0.attention.k_lin.bias', 'deberta.encoder.layer.19.attention.output.adapter.down.bias', 'deberta.encoder.layer.14.output.adapter.up.bias', 'deberta.encoder.layer.6.attention.output.adapter.down.weight', 'deberta.encoder.layer.19.attention.output.adapter.down.weight', 'deberta.encoder.layer.15.attention.output.adapter.down.weight', 'deberta.encoder.layer.23.attention.output.adapter.up.bias', 'deberta.encoder.layer.1.output.adapter.down.weight', 'trans_gauss.layer.0.output_layer_norm.weight', 'deberta.encoder.layer.7.attention.output.adapter.down.bias', 'deberta.encoder.layer.18.attention.output.adapter.up.bias', 'deberta.encoder.layer.22.attention.output.adapter.down.bias', 'deberta.encoder.layer.21.output.adapter.up.bias', 'deberta.encoder.layer.6.output.adapter.down.weight', 'trans_gauss.layer.0.ffn.lin2.weight', 'deberta.encoder.layer.10.output.adapter.down.bias', 'deberta.encoder.layer.0.attention.output.adapter.down.weight', 'deberta.encoder.layer.10.output.adapter.up.weight', 'trans_gauss.layer.0.output_layer_norm.bias', 'deberta.encoder.layer.2.output.adapter.down.bias', 'deberta.encoder.layer.5.output.adapter.up.bias', 'deberta.encoder.layer.1.attention.output.adapter.down.bias', 'deberta.encoder.layer.2.output.adapter.up.bias', 'deberta.encoder.layer.5.attention.output.adapter.up.bias', 'deberta.encoder.layer.21.attention.output.adapter.up.weight', 'deberta.encoder.layer.17.attention.output.adapter.up.bias', 'logit_gauss.1.bias', 'deberta.encoder.layer.17.attention.output.adapter.down.bias', 'deberta.encoder.layer.5.output.adapter.up.weight', 'deberta.encoder.layer.23.attention.output.adapter.up.weight', 'deberta.encoder.layer.0.attention.output.adapter.up.bias', 'deberta.encoder.layer.5.attention.output.adapter.down.weight', 'lm_predictions.lm_head.decoder.bias', 'deberta.encoder.layer.7.output.adapter.up.weight', 'deberta.encoder.layer.10.attention.output.adapter.down.bias', 'deberta.encoder.layer.6.output.adapter.up.bias', 'deberta.encoder.layer.4.output.adapter.up.bias', 'deberta.encoder.layer.4.output.adapter.down.weight', 'deberta.encoder.layer.13.output.adapter.down.weight', 'deberta.encoder.layer.20.output.adapter.up.bias', 'deberta.encoder.layer.17.output.adapter.down.bias', 'deberta.encoder.layer.0.output.adapter.down.weight', 'deberta.encoder.layer.11.output.adapter.down.weight', 'deberta.encoder.layer.22.output.adapter.down.weight', 'deberta.encoder.layer.12.attention.output.adapter.up.weight', 'deberta.encoder.layer.16.attention.output.adapter.up.weight', 'deberta.encoder.layer.8.attention.output.adapter.down.weight', 'deberta.encoder.layer.8.attention.output.adapter.down.bias', 'deberta.encoder.layer.17.output.adapter.down.weight', 'deberta.encoder.layer.9.output.adapter.up.weight', 'deberta.encoder.layer.22.output.adapter.down.bias', 'answer_embeddings.weight', 'deberta.encoder.layer.0.output.adapter.up.bias', 'deberta.encoder.layer.2.attention.output.adapter.down.bias', 'deberta.encoder.layer.18.output.adapter.down.weight', 'deberta.encoder.layer.7.output.adapter.up.bias', 'deberta.encoder.layer.7.attention.output.adapter.down.weight', 'deberta.encoder.layer.18.attention.output.adapter.up.weight', 'deberta.encoder.layer.10.attention.output.adapter.up.weight', 'deberta.encoder.layer.3.attention.output.adapter.down.bias', 'deberta.encoder.layer.21.attention.output.adapter.down.bias', 'deberta.encoder.layer.23.output.adapter.up.bias', 'deberta.encoder.layer.4.output.adapter.up.weight', 'deberta.encoder.layer.14.attention.output.adapter.up.bias', 'deberta.encoder.layer.5.output.adapter.down.weight', 'deberta.encoder.layer.21.output.adapter.down.weight', 'deberta.encoder.layer.19.output.adapter.up.weight', 'deberta.encoder.layer.14.output.adapter.down.weight', 'deberta.encoder.layer.3.attention.output.adapter.up.bias', 'deberta.embeddings.linear_video.bias', 'deberta.encoder.layer.13.output.adapter.up.weight', 'deberta.encoder.layer.21.output.adapter.up.weight', 'deberta.embeddings.linear_video.weight', 'deberta.encoder.layer.12.attention.output.adapter.up.bias', 'deberta.encoder.layer.11.attention.output.adapter.down.weight', 'deberta.encoder.layer.1.attention.output.adapter.up.bias', 'deberta.encoder.layer.2.attention.output.adapter.up.weight', 'deberta.encoder.layer.19.output.adapter.up.bias', 'deberta.encoder.layer.11.attention.output.adapter.up.weight', 'deberta.encoder.layer.17.attention.output.adapter.up.weight', 'deberta.encoder.layer.12.output.adapter.up.weight', 'deberta.encoder.layer.19.output.adapter.down.weight', 'trans_gauss.layer.0.attention.q_lin.weight', 'deberta.encoder.layer.9.output.adapter.down.weight', 'deberta.encoder.layer.1.output.adapter.down.bias', 'deberta.encoder.layer.17.output.adapter.up.bias', 'deberta.encoder.layer.8.output.adapter.up.weight', 'deberta.encoder.layer.23.output.adapter.down.weight', 'deberta.encoder.layer.14.attention.output.adapter.down.weight', 'deberta.encoder.layer.14.attention.output.adapter.down.bias', 'deberta.encoder.layer.11.output.adapter.up.bias', 'deberta.encoder.layer.10.attention.output.adapter.down.weight', 'deberta.encoder.layer.10.output.adapter.down.weight', 'deberta.encoder.layer.12.attention.output.adapter.down.bias', 'deberta.encoder.layer.19.attention.output.adapter.up.bias', 'deberta.encoder.layer.11.attention.output.adapter.up.bias', 'deberta.encoder.layer.0.attention.output.adapter.up.weight', 'trans_gauss.layer.0.attention.out_lin.bias', 'trans_gauss.layer.0.ffn.lin1.bias', 'deberta.encoder.layer.20.attention.output.adapter.up.bias', 'trans_gauss.layer.0.ffn.lin1.weight', 'deberta.encoder.layer.14.output.adapter.up.weight', 'deberta.encoder.layer.13.attention.output.adapter.up.bias', 'deberta.encoder.layer.16.output.adapter.down.weight', 'trans_gauss.layer.0.attention.v_lin.bias', 'deberta.encoder.layer.1.output.adapter.up.bias', 'deberta.encoder.layer.16.attention.output.adapter.down.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at ../../../data/pretrain_models/deberta-v2-xlarge and are newly initialized: ['deberta.encoder.layer.22.output.adapter.up.bias', 'deberta.encoder.layer.20.attention.output.adapter.down.bias', 'deberta.encoder.layer.20.output.adapter.down.bias', 'deberta.encoder.layer.21.attention.output.adapter.down.weight', 'deberta.encoder.layer.8.attention.output.adapter.down.weight', 'deberta.encoder.layer.16.attention.output.adapter.down.weight', 'deberta.encoder.layer.14.output.adapter.down.bias', 'deberta.encoder.layer.10.output.adapter.down.weight', 'deberta.encoder.layer.1.output.adapter.up.weight', 'deberta.encoder.layer.13.output.adapter.up.bias', 'deberta.encoder.layer.14.attention.output.adapter.up.bias', 'deberta.encoder.layer.18.attention.output.adapter.up.weight', 'deberta.encoder.layer.11.attention.output.adapter.down.weight', 'deberta.encoder.layer.16.output.adapter.up.bias', 'deberta.encoder.layer.7.attention.output.adapter.down.weight', 'answer_embeddings.weight', 'deberta.encoder.layer.9.output.adapter.up.weight', 'deberta.encoder.layer.3.output.adapter.up.bias', 'deberta.encoder.layer.9.attention.output.adapter.down.bias', 'deberta.encoder.layer.20.attention.output.adapter.up.bias', 'deberta.encoder.layer.5.attention.output.adapter.up.bias', 'deberta.encoder.layer.16.attention.output.adapter.down.bias', 'deberta.encoder.layer.1.attention.output.adapter.down.bias', 'deberta.encoder.layer.14.output.adapter.up.bias', 'deberta.encoder.layer.16.output.adapter.down.bias', 'deberta.encoder.layer.9.attention.output.adapter.up.weight', 'trans_gauss.layer.0.attention.k_lin.weight', 'deberta.encoder.layer.11.attention.output.adapter.up.bias', 'deberta.encoder.layer.2.output.adapter.up.weight', 'deberta.encoder.layer.10.output.adapter.up.weight', 'deberta.encoder.layer.2.output.adapter.up.bias', 'deberta.encoder.layer.15.attention.output.adapter.down.bias', 'lm_predictions.lm_head.decoder.weight', 'trans_gauss.layer.0.attention.v_lin.weight', 'deberta.encoder.layer.19.attention.output.adapter.up.weight', 'deberta.encoder.layer.23.output.adapter.down.weight', 'deberta.embeddings.linear_video.bias', 'answer_bias', 'deberta.encoder.layer.21.attention.output.adapter.down.bias', 'deberta.encoder.layer.15.output.adapter.up.bias', 'trans_gauss.layer.0.attention.v_lin.bias', 'deberta.encoder.layer.1.attention.output.adapter.down.weight', 'deberta.encoder.layer.2.attention.output.adapter.down.bias', 'deberta.encoder.layer.17.output.adapter.down.bias', 'deberta.encoder.layer.7.output.adapter.down.weight', 'deberta.encoder.layer.13.attention.output.adapter.down.weight', 'deberta.encoder.layer.19.output.adapter.down.weight', 'deberta.encoder.layer.23.output.adapter.up.bias', 'deberta.encoder.layer.22.output.adapter.down.bias', 'deberta.encoder.layer.12.attention.output.adapter.up.weight', 'deberta.encoder.layer.1.attention.output.adapter.up.weight', 'deberta.encoder.layer.8.attention.output.adapter.up.bias', 'deberta.encoder.layer.2.attention.output.adapter.down.weight', 'lm_predictions.lm_head.decoder.bias', 'deberta.encoder.layer.14.attention.output.adapter.down.weight', 'deberta.encoder.layer.19.attention.output.adapter.down.weight', 'trans_gauss.layer.0.sa_layer_norm.bias', 'deberta.encoder.layer.6.output.adapter.down.bias', 'trans_gauss.layer.0.attention.out_lin.weight', 'deberta.encoder.layer.1.output.adapter.up.bias', 'deberta.encoder.layer.0.output.adapter.down.weight', 'deberta.encoder.layer.12.attention.output.adapter.down.bias', 'deberta.encoder.layer.15.output.adapter.down.weight', 'deberta.encoder.layer.6.attention.output.adapter.down.bias', 'deberta.encoder.layer.3.output.adapter.down.weight', 'deberta.encoder.layer.18.output.adapter.down.weight', 'deberta.encoder.layer.4.attention.output.adapter.down.weight', 'deberta.encoder.layer.0.attention.output.adapter.up.bias', 'deberta.encoder.layer.9.output.adapter.down.weight', 'deberta.encoder.layer.12.output.adapter.down.weight', 'deberta.encoder.layer.13.output.adapter.down.weight', 'deberta.encoder.layer.15.output.adapter.down.bias', 'deberta.encoder.layer.5.attention.output.adapter.down.bias', 'deberta.encoder.layer.7.output.adapter.down.bias', 'deberta.encoder.layer.21.output.adapter.down.bias', 'deberta.encoder.layer.12.output.adapter.up.weight', 'trans_gauss.layer.0.attention.q_lin.bias', 'deberta.encoder.layer.17.attention.output.adapter.up.bias', 'deberta.encoder.layer.15.attention.output.adapter.down.weight', 'deberta.encoder.layer.23.attention.output.adapter.up.bias', 'deberta.encoder.layer.11.attention.output.adapter.down.bias', 'deberta.encoder.layer.11.output.adapter.down.bias', 'deberta.encoder.layer.20.attention.output.adapter.up.weight', 'deberta.encoder.layer.21.output.adapter.down.weight', 'deberta.encoder.layer.12.attention.output.adapter.down.weight', 'deberta.encoder.layer.14.output.adapter.up.weight', 'trans_gauss.layer.0.ffn.lin2.weight', 'deberta.encoder.layer.17.attention.output.adapter.up.weight', 'deberta.encoder.layer.6.output.adapter.down.weight', 'deberta.encoder.layer.14.output.adapter.down.weight', 'deberta.encoder.layer.3.output.adapter.up.weight', 'deberta.encoder.layer.0.attention.output.adapter.up.weight', 'deberta.encoder.layer.9.output.adapter.down.bias', 'deberta.encoder.layer.13.output.adapter.down.bias', 'deberta.encoder.layer.0.output.adapter.down.bias', 'deberta.encoder.layer.0.output.adapter.up.weight', 'deberta.encoder.layer.20.output.adapter.up.weight', 'deberta.encoder.layer.14.attention.output.adapter.down.bias', 'deberta.encoder.layer.20.attention.output.adapter.down.weight', 'deberta.encoder.layer.4.attention.output.adapter.up.weight', 'deberta.encoder.layer.23.attention.output.adapter.down.bias', 'deberta.encoder.layer.22.output.adapter.up.weight', 'deberta.encoder.layer.19.output.adapter.down.bias', 'deberta.encoder.layer.16.output.adapter.up.weight', 'deberta.encoder.layer.8.attention.output.adapter.up.weight', 'deberta.encoder.layer.4.output.adapter.up.bias', 'deberta.encoder.layer.15.attention.output.adapter.up.bias', 'trans_gauss.layer.0.output_layer_norm.bias', 'deberta.encoder.layer.1.attention.output.adapter.up.bias', 'deberta.encoder.layer.6.output.adapter.up.bias', 'deberta.encoder.layer.10.output.adapter.down.bias', 'deberta.encoder.layer.12.output.adapter.up.bias', 'deberta.encoder.layer.20.output.adapter.up.bias', 'deberta.encoder.layer.23.attention.output.adapter.down.weight', 'deberta.encoder.layer.10.output.adapter.up.bias', 'deberta.encoder.layer.17.output.adapter.up.bias', 'deberta.encoder.layer.21.attention.output.adapter.up.bias', 'deberta.encoder.layer.4.output.adapter.down.bias', 'deberta.encoder.layer.14.attention.output.adapter.up.weight', 'deberta.encoder.layer.22.output.adapter.down.weight', 'deberta.encoder.layer.0.attention.output.adapter.down.weight', 'deberta.encoder.layer.23.output.adapter.down.bias', 'deberta.encoder.layer.0.attention.output.adapter.down.bias', 'deberta.embeddings.linear_video.weight', 'deberta.encoder.layer.13.attention.output.adapter.up.weight', 'deberta.encoder.layer.2.output.adapter.down.bias', 'deberta.encoder.layer.5.output.adapter.up.bias', 'deberta.encoder.layer.17.output.adapter.down.weight', 'deberta.encoder.layer.13.attention.output.adapter.down.bias', 'deberta.encoder.layer.1.output.adapter.down.weight', 'deberta.encoder.layer.6.attention.output.adapter.up.weight', 'deberta.encoder.layer.16.output.adapter.down.weight', 'deberta.encoder.layer.11.output.adapter.up.weight', 'deberta.encoder.layer.5.output.adapter.up.weight', 'deberta.encoder.layer.19.output.adapter.up.bias', 'deberta.encoder.layer.9.attention.output.adapter.up.bias', 'deberta.encoder.layer.3.attention.output.adapter.down.bias', 'trans_gauss.layer.0.sa_layer_norm.weight', 'deberta.encoder.layer.10.attention.output.adapter.down.weight', 'deberta.encoder.layer.19.attention.output.adapter.up.bias', 'deberta.encoder.layer.3.attention.output.adapter.down.weight', 'deberta.encoder.layer.3.attention.output.adapter.up.weight', 'deberta.encoder.layer.21.output.adapter.up.weight', 'deberta.encoder.layer.3.attention.output.adapter.up.bias', 'deberta.encoder.layer.3.output.adapter.down.bias', 'trans_gauss.layer.0.output_layer_norm.weight', 'trans_gauss.layer.0.attention.out_lin.bias', 'deberta.encoder.layer.12.output.adapter.down.bias', 'deberta.encoder.layer.2.output.adapter.down.weight', 'deberta.encoder.layer.0.output.adapter.up.bias', 'deberta.encoder.layer.9.attention.output.adapter.down.weight', 'deberta.encoder.layer.12.attention.output.adapter.up.bias', 'deberta.encoder.layer.5.attention.output.adapter.down.weight', 'deberta.encoder.layer.7.attention.output.adapter.down.bias', 'deberta.encoder.layer.8.output.adapter.down.bias', 'deberta.encoder.layer.21.attention.output.adapter.up.weight', 'deberta.encoder.layer.18.output.adapter.up.bias', 'deberta.encoder.layer.6.output.adapter.up.weight', 'deberta.encoder.layer.18.attention.output.adapter.up.bias', 'deberta.encoder.layer.17.attention.output.adapter.down.bias', 'deberta.encoder.layer.4.attention.output.adapter.up.bias', 'deberta.encoder.layer.8.output.adapter.up.weight', 'deberta.encoder.layer.5.output.adapter.down.bias', 'deberta.encoder.layer.8.output.adapter.up.bias', 'deberta.encoder.layer.9.output.adapter.up.bias', 'deberta.encoder.layer.11.output.adapter.down.weight', 'trans_gauss.layer.0.attention.q_lin.weight', 'deberta.encoder.layer.22.attention.output.adapter.down.weight', 'deberta.encoder.layer.7.output.adapter.up.weight', 'deberta.encoder.layer.18.output.adapter.down.bias', 'deberta.encoder.layer.7.attention.output.adapter.up.bias', 'deberta.encoder.layer.13.output.adapter.up.weight', 'trans_gauss.layer.0.ffn.lin2.bias', 'deberta.encoder.layer.6.attention.output.adapter.up.bias', 'deberta.encoder.layer.7.output.adapter.up.bias', 'deberta.encoder.layer.18.attention.output.adapter.down.bias', 'deberta.encoder.layer.4.output.adapter.down.weight', 'deberta.encoder.layer.20.output.adapter.down.weight', 'deberta.encoder.layer.10.attention.output.adapter.up.weight', 'deberta.encoder.layer.23.output.adapter.up.weight', 'deberta.encoder.layer.6.attention.output.adapter.down.weight', 'deberta.encoder.layer.22.attention.output.adapter.up.bias', 'deberta.encoder.layer.17.output.adapter.up.weight', 'deberta.encoder.layer.16.attention.output.adapter.up.weight', 'deberta.encoder.layer.7.attention.output.adapter.up.weight', 'deberta.encoder.layer.10.attention.output.adapter.down.bias', 'deberta.encoder.layer.11.attention.output.adapter.up.weight', 'deberta.encoder.layer.2.attention.output.adapter.up.bias', 'deberta.encoder.layer.15.attention.output.adapter.up.weight', 'deberta.encoder.layer.5.output.adapter.down.weight', 'deberta.encoder.layer.21.output.adapter.up.bias', 'deberta.encoder.layer.10.attention.output.adapter.up.bias', 'trans_gauss.layer.0.ffn.lin1.bias', 'deberta.encoder.layer.16.attention.output.adapter.up.bias', 'deberta.encoder.layer.19.attention.output.adapter.down.bias', 'trans_gauss.layer.0.attention.k_lin.bias', 'logit_gauss.1.bias', 'deberta.encoder.layer.13.attention.output.adapter.up.bias', 'deberta.encoder.layer.1.output.adapter.down.bias', 'deberta.encoder.layer.19.output.adapter.up.weight', 'deberta.encoder.layer.17.attention.output.adapter.down.weight', 'deberta.encoder.layer.22.attention.output.adapter.down.bias', 'trans_gauss.layer.0.ffn.lin1.weight', 'deberta.encoder.layer.18.output.adapter.up.weight', 'deberta.encoder.layer.8.attention.output.adapter.down.bias', 'deberta.encoder.layer.11.output.adapter.up.bias', 'deberta.encoder.layer.22.attention.output.adapter.up.weight', 'deberta.encoder.layer.8.output.adapter.down.weight', 'deberta.encoder.layer.5.attention.output.adapter.up.weight', 'deberta.encoder.layer.23.attention.output.adapter.up.weight', 'deberta.encoder.layer.18.attention.output.adapter.down.weight', 'deberta.encoder.layer.15.output.adapter.up.weight', 'deberta.encoder.layer.4.attention.output.adapter.down.bias', 'deberta.encoder.layer.2.attention.output.adapter.up.weight', 'deberta.encoder.layer.4.output.adapter.up.weight', 'logit_gauss.1.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at ../../../data/pretrain_models/deberta-v2-xlarge and are newly initialized: ['deberta.encoder.layer.0.attention.output.adapter.up.weight', 'deberta.encoder.layer.7.attention.output.adapter.down.weight', 'deberta.encoder.layer.11.output.adapter.down.bias', 'deberta.encoder.layer.12.output.adapter.up.weight', 'deberta.encoder.layer.7.output.adapter.down.bias', 'deberta.encoder.layer.11.attention.output.adapter.up.bias', 'deberta.encoder.layer.10.attention.output.adapter.down.weight', 'deberta.encoder.layer.12.attention.output.adapter.down.bias', 'deberta.encoder.layer.15.output.adapter.up.weight', 'deberta.encoder.layer.6.attention.output.adapter.up.weight', 'deberta.encoder.layer.17.output.adapter.down.weight', 'trans_gauss.layer.0.attention.v_lin.bias', 'deberta.encoder.layer.1.output.adapter.up.bias', 'lm_predictions.lm_head.decoder.bias', 'deberta.encoder.layer.7.attention.output.adapter.up.weight', 'deberta.encoder.layer.4.attention.output.adapter.down.bias', 'deberta.encoder.layer.2.output.adapter.down.weight', 'deberta.encoder.layer.16.output.adapter.up.weight', 'deberta.encoder.layer.12.output.adapter.down.weight', 'trans_gauss.layer.0.ffn.lin2.bias', 'deberta.encoder.layer.17.output.adapter.up.weight', 'deberta.encoder.layer.6.output.adapter.down.weight', 'deberta.encoder.layer.2.output.adapter.down.bias', 'deberta.encoder.layer.4.attention.output.adapter.up.weight', 'deberta.encoder.layer.14.output.adapter.up.bias', 'deberta.encoder.layer.3.attention.output.adapter.down.bias', 'deberta.encoder.layer.23.attention.output.adapter.up.weight', 'deberta.encoder.layer.3.output.adapter.down.bias', 'deberta.encoder.layer.17.output.adapter.down.bias', 'deberta.encoder.layer.21.attention.output.adapter.up.weight', 'deberta.encoder.layer.11.attention.output.adapter.up.weight', 'deberta.encoder.layer.6.attention.output.adapter.down.bias', 'deberta.encoder.layer.10.attention.output.adapter.up.weight', 'deberta.encoder.layer.16.output.adapter.down.weight', 'deberta.encoder.layer.14.attention.output.adapter.up.weight', 'deberta.encoder.layer.2.output.adapter.up.bias', 'deberta.encoder.layer.5.attention.output.adapter.up.weight', 'deberta.encoder.layer.19.output.adapter.down.weight', 'deberta.encoder.layer.2.attention.output.adapter.down.bias', 'deberta.encoder.layer.4.attention.output.adapter.up.bias', 'trans_gauss.layer.0.attention.out_lin.weight', 'deberta.encoder.layer.21.attention.output.adapter.down.weight', 'deberta.encoder.layer.9.output.adapter.up.bias', 'deberta.encoder.layer.23.attention.output.adapter.down.bias', 'trans_gauss.layer.0.ffn.lin1.bias', 'deberta.encoder.layer.21.output.adapter.down.bias', 'deberta.encoder.layer.6.attention.output.adapter.up.bias', 'answer_bias', 'deberta.encoder.layer.14.attention.output.adapter.up.bias', 'deberta.encoder.layer.19.output.adapter.up.bias', 'deberta.encoder.layer.13.attention.output.adapter.down.weight', 'deberta.encoder.layer.23.output.adapter.up.weight', 'deberta.encoder.layer.8.output.adapter.up.weight', 'trans_gauss.layer.0.attention.q_lin.weight', 'deberta.encoder.layer.9.attention.output.adapter.down.weight', 'deberta.encoder.layer.10.attention.output.adapter.up.bias', 'deberta.encoder.layer.0.output.adapter.up.bias', 'deberta.encoder.layer.18.output.adapter.up.weight', 'deberta.encoder.layer.15.attention.output.adapter.up.bias', 'deberta.encoder.layer.19.output.adapter.down.bias', 'deberta.encoder.layer.15.output.adapter.down.weight', 'deberta.encoder.layer.8.output.adapter.up.bias', 'deberta.encoder.layer.13.attention.output.adapter.up.weight', 'deberta.encoder.layer.9.output.adapter.up.weight', 'deberta.encoder.layer.20.attention.output.adapter.up.bias', 'deberta.encoder.layer.6.attention.output.adapter.down.weight', 'deberta.encoder.layer.18.attention.output.adapter.down.bias', 'deberta.encoder.layer.2.attention.output.adapter.up.bias', 'deberta.encoder.layer.2.attention.output.adapter.up.weight', 'deberta.encoder.layer.18.output.adapter.down.weight', 'deberta.encoder.layer.21.attention.output.adapter.down.bias', 'deberta.encoder.layer.13.output.adapter.down.bias', 'deberta.encoder.layer.21.output.adapter.down.weight', 'deberta.encoder.layer.7.output.adapter.up.bias', 'deberta.encoder.layer.14.attention.output.adapter.down.bias', 'deberta.encoder.layer.16.attention.output.adapter.down.weight', 'deberta.encoder.layer.9.output.adapter.down.bias', 'deberta.encoder.layer.0.attention.output.adapter.up.bias', 'deberta.encoder.layer.4.output.adapter.up.bias', 'deberta.encoder.layer.4.attention.output.adapter.down.weight', 'deberta.encoder.layer.22.attention.output.adapter.down.weight', 'deberta.encoder.layer.18.attention.output.adapter.up.weight', 'deberta.encoder.layer.18.attention.output.adapter.down.weight', 'deberta.encoder.layer.23.attention.output.adapter.up.bias', 'deberta.encoder.layer.7.output.adapter.up.weight', 'trans_gauss.layer.0.sa_layer_norm.bias', 'deberta.encoder.layer.13.attention.output.adapter.down.bias', 'deberta.encoder.layer.18.output.adapter.down.bias', 'deberta.encoder.layer.13.output.adapter.up.bias', 'deberta.encoder.layer.4.output.adapter.down.weight', 'deberta.encoder.layer.15.attention.output.adapter.down.weight', 'deberta.encoder.layer.20.output.adapter.down.bias', 'deberta.encoder.layer.8.attention.output.adapter.down.bias', 'logit_gauss.1.weight', 'deberta.encoder.layer.23.attention.output.adapter.down.weight', 'deberta.encoder.layer.13.attention.output.adapter.up.bias', 'deberta.encoder.layer.10.attention.output.adapter.down.bias', 'deberta.encoder.layer.11.attention.output.adapter.down.weight', 'deberta.encoder.layer.8.output.adapter.down.weight', 'deberta.encoder.layer.10.output.adapter.down.weight', 'deberta.encoder.layer.2.output.adapter.up.weight', 'lm_predictions.lm_head.decoder.weight', 'deberta.encoder.layer.12.output.adapter.down.bias', 'deberta.encoder.layer.23.output.adapter.down.bias', 'deberta.encoder.layer.7.output.adapter.down.weight', 'logit_gauss.1.bias', 'deberta.encoder.layer.19.attention.output.adapter.up.weight', 'deberta.encoder.layer.9.attention.output.adapter.down.bias', 'deberta.encoder.layer.19.attention.output.adapter.down.weight', 'deberta.encoder.layer.1.output.adapter.up.weight', 'deberta.encoder.layer.13.output.adapter.down.weight', 'deberta.encoder.layer.22.output.adapter.down.weight', 'deberta.encoder.layer.1.attention.output.adapter.down.bias', 'deberta.encoder.layer.14.output.adapter.down.bias', 'deberta.encoder.layer.12.attention.output.adapter.up.bias', 'deberta.encoder.layer.22.attention.output.adapter.up.weight', 'deberta.encoder.layer.17.attention.output.adapter.up.weight', 'deberta.encoder.layer.15.output.adapter.up.bias', 'trans_gauss.layer.0.ffn.lin2.weight', 'deberta.encoder.layer.21.attention.output.adapter.up.bias', 'deberta.encoder.layer.16.output.adapter.up.bias', 'deberta.encoder.layer.20.output.adapter.down.weight', 'deberta.encoder.layer.19.attention.output.adapter.down.bias', 'deberta.encoder.layer.1.attention.output.adapter.up.weight', 'deberta.encoder.layer.11.attention.output.adapter.down.bias', 'deberta.encoder.layer.11.output.adapter.up.weight', 'deberta.encoder.layer.0.attention.output.adapter.down.bias', 'deberta.encoder.layer.0.output.adapter.up.weight', 'deberta.encoder.layer.5.attention.output.adapter.down.weight', 'trans_gauss.layer.0.attention.v_lin.weight', 'deberta.encoder.layer.1.attention.output.adapter.down.weight', 'deberta.encoder.layer.17.output.adapter.up.bias', 'deberta.encoder.layer.22.output.adapter.up.bias', 'deberta.encoder.layer.5.output.adapter.down.weight', 'deberta.encoder.layer.22.attention.output.adapter.up.bias', 'deberta.encoder.layer.2.attention.output.adapter.down.weight', 'deberta.encoder.layer.15.attention.output.adapter.up.weight', 'deberta.encoder.layer.5.attention.output.adapter.up.bias', 'deberta.encoder.layer.9.attention.output.adapter.up.weight', 'deberta.encoder.layer.12.output.adapter.up.bias', 'deberta.encoder.layer.19.output.adapter.up.weight', 'trans_gauss.layer.0.sa_layer_norm.weight', 'deberta.encoder.layer.19.attention.output.adapter.up.bias', 'deberta.encoder.layer.0.output.adapter.down.weight', 'deberta.encoder.layer.16.attention.output.adapter.down.bias', 'deberta.encoder.layer.14.output.adapter.up.weight', 'deberta.encoder.layer.3.output.adapter.up.bias', 'answer_embeddings.weight', 'deberta.encoder.layer.16.attention.output.adapter.up.weight', 'deberta.encoder.layer.5.output.adapter.down.bias', 'deberta.encoder.layer.23.output.adapter.down.weight', 'deberta.encoder.layer.0.attention.output.adapter.down.weight', 'deberta.encoder.layer.5.output.adapter.up.bias', 'deberta.encoder.layer.18.attention.output.adapter.up.bias', 'trans_gauss.layer.0.attention.k_lin.weight', 'deberta.encoder.layer.9.output.adapter.down.weight', 'deberta.encoder.layer.20.attention.output.adapter.down.bias', 'deberta.encoder.layer.23.output.adapter.up.bias', 'deberta.encoder.layer.21.output.adapter.up.bias', 'deberta.encoder.layer.1.attention.output.adapter.up.bias', 'deberta.encoder.layer.1.output.adapter.down.bias', 'deberta.encoder.layer.10.output.adapter.down.bias', 'deberta.encoder.layer.5.output.adapter.up.weight', 'deberta.encoder.layer.22.attention.output.adapter.down.bias', 'deberta.encoder.layer.0.output.adapter.down.bias', 'deberta.encoder.layer.22.output.adapter.down.bias', 'deberta.encoder.layer.4.output.adapter.up.weight', 'deberta.encoder.layer.14.output.adapter.down.weight', 'deberta.encoder.layer.20.output.adapter.up.weight', 'deberta.encoder.layer.6.output.adapter.up.bias', 'deberta.encoder.layer.7.attention.output.adapter.down.bias', 'deberta.embeddings.linear_video.weight', 'deberta.encoder.layer.11.output.adapter.down.weight', 'deberta.encoder.layer.6.output.adapter.down.bias', 'deberta.encoder.layer.12.attention.output.adapter.up.weight', 'deberta.encoder.layer.8.attention.output.adapter.up.bias', 'deberta.encoder.layer.15.attention.output.adapter.down.bias', 'deberta.encoder.layer.7.attention.output.adapter.up.bias', 'deberta.encoder.layer.20.output.adapter.up.bias', 'trans_gauss.layer.0.ffn.lin1.weight', 'deberta.encoder.layer.17.attention.output.adapter.down.weight', 'deberta.encoder.layer.20.attention.output.adapter.up.weight', 'deberta.encoder.layer.5.attention.output.adapter.down.bias', 'trans_gauss.layer.0.attention.q_lin.bias', 'trans_gauss.layer.0.output_layer_norm.bias', 'trans_gauss.layer.0.attention.out_lin.bias', 'trans_gauss.layer.0.output_layer_norm.weight', 'deberta.encoder.layer.17.attention.output.adapter.down.bias', 'deberta.encoder.layer.13.output.adapter.up.weight', 'deberta.encoder.layer.3.attention.output.adapter.down.weight', 'deberta.encoder.layer.8.attention.output.adapter.up.weight', 'deberta.encoder.layer.4.output.adapter.down.bias', 'deberta.encoder.layer.12.attention.output.adapter.down.weight', 'deberta.encoder.layer.20.attention.output.adapter.down.weight', 'deberta.encoder.layer.14.attention.output.adapter.down.weight', 'deberta.encoder.layer.10.output.adapter.up.bias', 'deberta.encoder.layer.15.output.adapter.down.bias', 'deberta.encoder.layer.10.output.adapter.up.weight', 'deberta.encoder.layer.16.attention.output.adapter.up.bias', 'deberta.encoder.layer.21.output.adapter.up.weight', 'deberta.encoder.layer.3.attention.output.adapter.up.bias', 'deberta.encoder.layer.6.output.adapter.up.weight', 'deberta.encoder.layer.22.output.adapter.up.weight', 'deberta.encoder.layer.11.output.adapter.up.bias', 'deberta.encoder.layer.8.attention.output.adapter.down.weight', 'deberta.embeddings.linear_video.bias', 'deberta.encoder.layer.9.attention.output.adapter.up.bias', 'deberta.encoder.layer.3.output.adapter.down.weight', 'deberta.encoder.layer.18.output.adapter.up.bias', 'deberta.encoder.layer.1.output.adapter.down.weight', 'deberta.encoder.layer.3.attention.output.adapter.up.weight', 'deberta.encoder.layer.16.output.adapter.down.bias', 'deberta.encoder.layer.3.output.adapter.up.weight', 'trans_gauss.layer.0.attention.k_lin.bias', 'deberta.encoder.layer.17.attention.output.adapter.up.bias', 'deberta.encoder.layer.8.output.adapter.down.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
number of params: 43909634
Load ../../../data/nextgqa/CLIPL/CLIPL_I_val.h5...
(570, 32, 768)
Val Samples:3358
Load ../../../data/nextgqa/CLIPL/CLIPL_I_train.h5...
(3870, 32, 768)
Train Samples:34132
loading from ../../../data/pretrain_models/FBLM/frozenbilm.pth
Start training
Starting epoch 0
Epoch: [0]  [   0/5689]  eta: 2:37:21  loss: 0.8503 (0.8503)  cls_loss: 0.8503 (0.8503)  lr: 0.0000 (0.0000)  time: 1.6596  data: 0.2638  max mem: 14465
Epoch: [0]  [2000/5689]  eta: 0:34:36  loss: 0.6284 (0.6920)  cls_loss: 0.6284 (0.6920)  lr: 0.0000 (0.0000)  time: 0.5617  data: 0.0012  max mem: 16681
Epoch: [0]  [4000/5689]  eta: 0:15:49  loss: 0.4878 (0.6192)  cls_loss: 0.4878 (0.6192)  lr: 0.0000 (0.0000)  time: 0.5566  data: 0.0012  max mem: 16681
Epoch: [0]  [5688/5689]  eta: 0:00:00  loss: 0.4569 (0.5757)  cls_loss: 0.4569 (0.5757)  lr: 0.0000 (0.0000)  time: 0.5610  data: 0.0011  max mem: 16681
Epoch: [0] Total time: 0:53:22 (0.5630 s / it)
Averaged stats: loss: 0.4569 (0.5757)  cls_loss: 0.4569 (0.5757)  lr: 0.0000 (0.0000)
Validating nextgqa
val:  [ 0/53]  eta: 0:01:32  acc: 0.6250 (0.6250)  time: 1.7390  data: 0.3117  max mem: 16681
val:  [52/53]  eta: 0:00:01  acc: 0.6094 (0.6112)  time: 1.4140  data: 0.0013  max mem: 16681
val: Total time: 0:01:16 (1.4370 s / it)
nextgqa
val acc:  60.96%
Training time 0:54:58
Starting epoch 1
Epoch: [1]  [   0/5689]  eta: 1:11:05  loss: 0.4402 (0.4402)  cls_loss: 0.4402 (0.4402)  lr: 0.0000 (0.0000)  time: 0.7497  data: 0.1893  max mem: 16681
